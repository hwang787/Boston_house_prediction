{
  "cells": [
    {
      "metadata": {
        "_uuid": "fac9ef36d778ebb38132fdb6ea1b15ca024f1b8d",
        "_cell_guid": "b494348f-587c-43f2-9847-5f4ab82a620c"
      },
      "cell_type": "markdown",
      "source": "***Predicting Boston Housing Prices***"
    },
    {
      "metadata": {
        "_uuid": "220a2b500271b9028ea4db9f86ed895c1a108418",
        "_cell_guid": "3380bc51-c5b2-4888-af4b-7c41ea404f9f"
      },
      "cell_type": "markdown",
      "source": "In this example, you will apply basic machine learning concepts on data collected for housing prices in the Boston, Massachusetts area to predict the selling price of a new home. You will first explore the data to obtain important features and descriptive statistics about the dataset. Next, you will properly split the data into testing and training subsets, and determine a suitable performance metric for this problem. You will then analyze performance graphs for a learning algorithm with varying parameters and training set sizes. This will enable you to pick the optimal model that best generalizes for unseen data. Finally, you will test this optimal model on a new sample and compare the predicted selling price to your statistics."
    },
    {
      "metadata": {
        "_uuid": "c4dbe77e30e7b20c8c5c741835c276f1081d007f",
        "_cell_guid": "f7781f93-9149-4b4b-b92b-d79214ab4f36",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.cross_validation import ShuffleSplit\n\n# Pretty display for notebooks\n%matplotlib inline\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2faa55da78163b29b3049c147b54b58d7d85363",
        "_cell_guid": "51c70917-2b84-45ee-9a82-38d62bd59b9b"
      },
      "cell_type": "markdown",
      "source": "First, you need to load the file (CSV)."
    },
    {
      "metadata": {
        "_uuid": "5c7fd565ce4f1dc8476fe57d2f35b83aa8729e2c",
        "_cell_guid": "8e25d9c5-6563-49c7-8354-e399aa64b2e7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Load the Boston housing dataset\ndata = pd.read_csv('../input/housing.csv')\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\n\n# Success\nprint (\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2ac2acd4a8ca239a250bbc5a70cb3aa2077ef21",
        "_cell_guid": "1dff8c6b-8258-47f0-b851-9abda268bde4"
      },
      "cell_type": "markdown",
      "source": "It is important to calculate the minimum, maximum, mean, median, and standard deviation of 'MEDV', which is stored in prices. Store each calculation in their respective variable."
    },
    {
      "metadata": {
        "_uuid": "a96a14c26c90860897919dc6c26857cce00a217d",
        "_cell_guid": "56224d8b-9fdf-41c1-a5da-5348d06a3205",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO: Minimum price of the data\nminimum_price = np.min(prices)\n\n# TODO: Maximum price of the data\nmaximum_price = np.max(prices)\n\n# TODO: Mean price of the data\nmean_price = np.mean(prices)\n\n# TODO: Median price of the data\nmedian_price = np.median(prices)\n\n# TODO: Standard deviation of prices of the data\nstd_price = np.std(prices)\n\n# Show the calculated statistics\nprint (\"Statistics for Boston housing dataset:\\n\")\nprint (\"Minimum price: ${:,.2f}\".format(minimum_price))\nprint (\"Maximum price: ${:,.2f}\".format(maximum_price))\nprint (\"Mean price: ${:,.2f}\".format(mean_price))\nprint (\"Median price ${:,.2f}\".format(median_price))\nprint (\"Standard deviation of prices: ${:,.2f}\".format(std_price))\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3429fb8b1ec0af7c55333e4b9ebc54f939742174",
        "_cell_guid": "2816f6c2-3b8c-43de-8f78-37d6ac263b61"
      },
      "cell_type": "markdown",
      "source": "Create us some graphs to see the data behavior"
    },
    {
      "metadata": {
        "_uuid": "15092fdec736b755db14261a6bcf2154e0249190",
        "_cell_guid": "aaf72025-4f94-4766-9565-15a67a72864f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor var in ['RM', 'LSTAT', 'PTRATIO']:\n    sns.regplot(data[var], prices)\n    plt.show()\n    \n# Using pyplot\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20, 5))\n\n# i: index\nfor i, col in enumerate(features.columns):\n    # 3 plots here hence 1, 3\n    plt.subplot(1, 3, i+1)\n    x = data[col]\n    y = prices\n    plt.plot(x, y, 'o')\n    # Create regression line\n    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('prices')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c15ce87ca51e195570859c5df9b32d10a90e31a",
        "_cell_guid": "b3d613c0-c187-4bf1-b718-305ab90e14e0"
      },
      "cell_type": "markdown",
      "source": "Use r2_score from sklearn.metrics to perform a performance calculation between y_true and y_predict."
    },
    {
      "metadata": {
        "_uuid": "42c4ed482e16da789639e2eff03681541b314bee",
        "_cell_guid": "7efe0020-3943-4291-8cdd-faf8878b9b30",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO: Import 'r2_score'\nfrom sklearn.metrics import r2_score\n\n\"\"\" Calculates the performance score between true and predicted values based on the metric chosen. \"\"\"\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between\n        true and predicted values based on the metric chosen. \"\"\"\n\n    # Calculate the performance score between 'y_true' and 'y_predict'\n    score = r2_score(y_true, y_predict, multioutput='variance_weighted')\n\n    # Return the score\n    return score\n\n# TODO: Calculate the performance score between 'y_true' and 'y_predict'\ny_true = [3, -0.5, 2, 7, 4.2]\ny_predict = [2.5, 0.0, 2.1, 7.8, 5.3]\n\nscore = performance_metric(y_true, y_predict)\n\nprint (\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))\n\n# Conclution: there seems to be a high (92%) correlation between the predicted and true values \n# of the target value, and the model nearly predicts the target variables.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7161a6be562bf9125b2a989b43db63efd27c853b",
        "_cell_guid": "2566a493-0fb6-4b35-94aa-ab516be0d76f"
      },
      "cell_type": "markdown",
      "source": "Your next implementation requires that you take the Boston housing dataset and split the data into training and testing subsets. "
    },
    {
      "metadata": {
        "_uuid": "a9b5895ed331ec17dd5927d49cd46b2f2f40427f",
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "_cell_guid": "41c96871-eb19-42fa-93c9-300c0b082cc9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO: Import 'train_test_split'\nfrom sklearn.cross_validation import train_test_split\n\n# TODO: Shuffle and split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=10)\n\n# Success\nprint (\"Training and testing split was successful.\")\n\n# Check if split is actually correct\n# We can see it's roughly 80% train and 20% train\n# So we can proceed!\nprint(features.shape[0])\nprint(float(X_train.shape[0]) / float(features.shape[0]))\nprint(float(X_test.shape[0]) / float(features.shape[0]))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85d749b905f1dfaaae452627a9c9bc4fd26ac8c4",
        "_cell_guid": "a5f398ee-20ea-4dc5-82c2-7663615f6284"
      },
      "cell_type": "markdown",
      "source": "Code to create visualizations"
    },
    {
      "metadata": {
        "_uuid": "821164a4f2bed4a7eaac8c00cd4e2c9b776f1b1e",
        "_cell_guid": "71368abe-eb58-4c80-a6b2-0dff30c9d4df",
        "trusted": false
      },
      "cell_type": "code",
      "source": "###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport numpy as np\nimport sklearn.learning_curve as curves\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.cross_validation import ShuffleSplit, train_test_split\n\ndef ModelLearning(X, y):\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. \"\"\"\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on max_depth\n    for k, depth in enumerate([1,3,6,10]):\n        \n        # Create a Decision tree regressor at max_depth = depth\n        regressor = DecisionTreeRegressor(max_depth = depth)\n\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = curves.learning_curve(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n        \n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve \n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = 'r')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = 'g')\n        \n        # Labels\n        ax.set_title('max_depth = %s'%(depth))\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n    \n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()\n\n\ndef ModelComplexity(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(1,11)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = curves.validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = \"max_depth\", param_range = max_depth, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title('Decision Tree Regressor Complexity Performance')\n    pl.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n    pl.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    pl.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    pl.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n    \n    # Visual aesthetics\n    pl.legend(loc = 'lower right')\n    pl.xlabel('Maximum Depth')\n    pl.ylabel('Score')\n    pl.ylim([-0.05,1.05])\n    pl.show()\n\ndef PredictTrials(X, y, fitter, data):\n    \"\"\" Performs trials of fitting and predicting data. \"\"\"\n\n    # Store the predicted prices\n    prices = []\n\n    for k in range(10):\n        # Split the data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n            test_size = 0.2, random_state = k)\n        \n        # Fit the data\n        reg = fitter(X_train, y_train)\n        \n        # Make a prediction\n        pred = reg.predict([data[0]])[0]\n        prices.append(pred)\n        \n        # Result\n        print (\"Trial {}: ${:,.2f}\".format(k+1, pred))\n\n    # Display price range\n    print (\"\\nRange in prices: ${:,.2f}\".format(max(prices) - min(prices)))\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b055a3538e50fd667494f2231fce1df8a14fc7d1",
        "_cell_guid": "43ff05ea-8685-480f-989b-3f309d7a5594"
      },
      "cell_type": "markdown",
      "source": "The following code cell produces four graphs for a decision tree model with different maximum depths. Each graph visualizes the learning curves of the model for both training and testing as the size of the training set is increased. "
    },
    {
      "metadata": {
        "_uuid": "9974ad2cca4d9db25715f019c8641d9533d9dc98",
        "_cell_guid": "3e412eab-c02e-42bb-a890-786bca4feff5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Produce learning curves for varying training set sizes and maximum depths\nModelLearning(features, prices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a3e5b2a19b18eda6f1e288a0ee41b89d8dd17cb",
        "_cell_guid": "f2ceade5-3b41-4fe9-8101-6863c260153d"
      },
      "cell_type": "markdown",
      "source": "The following code cell produces a graph for a decision tree model that has been trained and validated on the training data using different maximum depths. The graph produces two complexity curves — one for training and one for validation. Similar to the learning curves, the shaded regions of both the complexity curves denote the uncertainty in those curves, and the model is scored on both the training and validation sets using the performance_metric function."
    },
    {
      "metadata": {
        "_uuid": "20ebdc1579f33589262a39baa0c6bd8ede143f8a",
        "_cell_guid": "84952838-6d8d-46dd-a2e8-fb9f043a2c06",
        "trusted": false
      },
      "cell_type": "code",
      "source": "ModelComplexity(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb854b8525290746f3d19ab773930772b58d3e40",
        "_cell_guid": "1f1069d7-1aae-4a77-be23-56dfc06f0122"
      },
      "cell_type": "markdown",
      "source": "Your final implementation requires that you bring everything together and train a model using the decision tree algorithm. To ensure that you are producing an optimized model, you will train the model using the grid search technique to optimize the 'max_depth' parameter for the decision tree. The 'max_depth' parameter can be thought of as how many questions the decision tree algorithm is allowed to ask about the data before making a prediction. Decision trees are part of a class of algorithms called supervised learning algorithms.\n\nUsing **GridSearchCV:**"
    },
    {
      "metadata": {
        "_uuid": "dcd2715270e4528d198769374ae44ac0054d6f09",
        "_cell_guid": "b2bc6dbe-cf99-4065-9ec2-99fdd9202fcc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\nfrom sklearn.metrics import make_scorer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.grid_search import GridSearchCV\n\ndef fit_model(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \"\"\"\n\n    # Create cross-validation sets from the training data\n    # ShuffleSplit works iteratively compared to KFOLD\n    # It saves computation time when your dataset grows\n    # X.shape[0] is the total number of elements\n    # n_iter is the number of re-shuffling & splitting iterations.\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    # Instantiate\n    regressor = DecisionTreeRegressor(random_state=0)\n\n    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    dt_range = range(1, 11)\n    #params = dict(max_depth=dt_range)\n    params = {'max_depth': list(range(1,11))}\n    \n    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n    # We initially created performance_metric using R2_score\n    scoring_fnc = make_scorer(performance_metric)\n\n    # TODO: Create the grid search object\n    # You would realize we manually created each, including scoring_func using R^2\n    grid = GridSearchCV(regressor, params, cv=cv_sets, scoring=scoring_fnc)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c663cd9eee0dc881efb61e7db4db17a11d4d8637",
        "_cell_guid": "672b6a1d-a096-498a-8b97-6ddf60306d13"
      },
      "cell_type": "markdown",
      "source": "Using **RandomizedSearchCV :**"
    },
    {
      "metadata": {
        "_uuid": "a5f3f6a88469d3db0bf392571ec369c6e6c21337",
        "collapsed": true,
        "_cell_guid": "9292dfa8-f745-44d7-b4b5-88c16c21a2a7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Import RandomizedSearchCV\nfrom sklearn.grid_search import RandomizedSearchCV\n\n# Create new similar function\ndef fit_model_2(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \"\"\"\n\n    # Create cross-validation sets from the training data\n    # ShuffleSplit works iteratively compared to KFOLD\n    # It saves computation time when your dataset grows\n    # X.shape[0] is the total number of elements\n    # n_iter is the number of re-shuffling & splitting iterations.\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    # Instantiate\n    regressor = DecisionTreeRegressor(random_state=0)\n\n    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    dt_range = range(1, 11)\n    #params = dict(max_depth=dt_range)\n    params = {'max_depth': list(range(1,11))}\n    \n    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n    # We initially created performance_metric using R2_score\n    scoring_fnc = make_scorer(performance_metric)\n\n    # TODO: Create the grid search object\n    # You would realize we manually created each, including scoring_func using R^2\n    rand = RandomizedSearchCV(regressor, params, cv=cv_sets, scoring=scoring_fnc)\n\n    # Fit the grid search object to the data to compute the optimal model\n    rand = rand.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return rand.best_estimator_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04606d7299621a84957d9393146437596b5055a2",
        "_cell_guid": "d06e9a40-b892-4a92-8e1f-5937a99839e2"
      },
      "cell_type": "markdown",
      "source": "Once a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data. In the case of a decision tree regressor, the model has learned what the best questions to ask about the input data are, and can respond with a prediction for the target variable. You can use these predictions to gain information about data where the value of the target variable is unknown — such as data the model was not trained on."
    },
    {
      "metadata": {
        "_uuid": "2d381603624f988bb71c041260f2fb379934a957",
        "_cell_guid": "f0ae013a-a9a4-4972-8f28-ec136a5c39eb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Fit the training data to the model using grid search\nreg = fit_model(X_train, y_train)\n\n# Produce the value for 'max_depth'\nprint (\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))\n\nprint (reg.get_params())\n\n# We can access our value from reg.get_params(), a dictionary, using dict['key']\nreg.get_params()['max_depth']\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04c05ad9d454934e6cbc9cd01ee9197a204af3bd",
        "_cell_guid": "9b772989-dd3e-44c3-b386-f7f091bb1de2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Fit the training data to the model using randomized grid search\nreg_2 = fit_model_2(X_train, y_train)\n\n# Produce the value for 'max_depth'\nprint (\"Parameter 'max_depth' is {} for the optimal model.\".format(reg_2.get_params()['max_depth']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e695f706b1844ee7da3f3ad501f885679d842ee",
        "_cell_guid": "a9edc895-f574-4fc6-a69b-12fbfe8d160e"
      },
      "cell_type": "markdown",
      "source": "Imagine that you were a real estate agent in the Boston area looking to use this model to help price homes owned by your clients that they wish to sell."
    },
    {
      "metadata": {
        "_uuid": "e74c6e898cc0e3e1c9a4afe6d89f21244e41d202",
        "_cell_guid": "7bdc5f0b-8132-4ddd-9888-d1d5878381aa",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Produce a matrix for client data\nclient_data = [[5, 17, 15], # Client 1\n               [4, 32, 22], # Client 2\n               [8, 3, 12]]  # Client 3\n\n# Show predictions\nfor i, price in enumerate(reg.predict(client_data)):\n    print (\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f1e0aaa4ee7417f4bbf840ecd96d73f3b166ebb",
        "_cell_guid": "a6791504-ec78-413b-8682-9bbc5964650d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\nplt.hist(prices, bins = 20)\nfor price in reg.predict(client_data):\n    plt.axvline(price, lw = 5, c = 'r')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cdd36371774998b822f720f10481f88c0a13769a"
      },
      "cell_type": "markdown",
      "source": " Prediction using **NearestNeighbors:**"
    },
    {
      "metadata": {
        "_uuid": "3c02dda89d18c28b029d778ceeecb9d94ee900ed",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Import NearestNeighbors\nfrom sklearn.neighbors import NearestNeighbors\n\n# Set number of neighbors\nnum_neighbors=5\n\ndef nearest_neighbor_price(x):\n    # x is your vector and X is the data set.\n    def find_nearest_neighbor_indexes(x, X):\n        # Instantiate\n        neigh = NearestNeighbors(num_neighbors)\n        # Fit\n        neigh.fit(X)\n        distance, indexes = neigh.kneighbors(x)\n        return indexes\n        # This returns, the position, say for example [4, 55, 22]\n        # array([[357, 397, 356, 141, 395]])\n    indexes = find_nearest_neighbor_indexes(x, features)\n    # Create list\n    sum_prices = []\n    # Loop through the array\n    for i in indexes:\n        # Append the prices to the list using the index position i\n        sum_prices.append(prices[i])\n    # Average prices\n    neighbor_avg = np.mean(sum_prices)\n    # Return average\n    return neighbor_avg\n\n# Test if it's working with a list [4, 55, 22]\narr_test = np.array([4, 55, 22]).reshape(1, -1)\nprint (nearest_neighbor_price(arr_test))\n\n# client_data = [[5, 17, 15], # Client 1\n               #[4, 32, 22], # Client 2\n               #[8, 3, 12]]  # Client 3\n\n# Loop through data, this is basically doing the following\n# print(nearest_neighbor_price([5, 17, 15]))\n# print(nearest_neighbor_price([4, 32, 22]))\n# print(nearest_neighbor_price([8, 3, 12]]))\nindex = 0\nfor i in client_data:\n    arr = np.array(i).reshape(1, -1)\n    val=nearest_neighbor_price(arr)\n    index += 1\n    # num_neighbours is constant at 5\n    # index changes from 1 to 2 to 3\n    # value changes respectively from $372,540.00 to $162,120.00 to $897,120.00\n    print (\"The predicted {} nearest neighbors price for home {} is: ${:,.2f}\".format(num_neighbors,index, val))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a46f8ed5affab7ea47cb2cf3a7386f4679ff406",
        "trusted": true
      },
      "cell_type": "code",
      "source": "PredictTrials(features, prices, fit_model, client_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3194db88c3bde103e058cc5a84822288a1b39366"
      },
      "cell_type": "markdown",
      "source": "Source: Udacity course"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}